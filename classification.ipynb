{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os as os\n",
    "import sys as sys\n",
    "import re as re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json as json\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.min_rows = None\n",
    "pd.options.display.max_rows = 20\n",
    "pd.options.display.max_colwidth = 1000\n",
    "\n",
    "from config import RANDOM_STATE, MATERIALS_PATH, RESULTS_PATH\n",
    "\n",
    "GENE_SYMBOL_EFFECT_SIZE = MATERIALS_PATH.joinpath('capstone_body_weight_Statistical_effect_size_analysis_genotype_early_adult_scaled_13022023_gene_symbol_harmonized.pkl')\n",
    "PROTEIN_SEQUENCE_PATH = MATERIALS_PATH.joinpath('gene_symbol_protein_sequences.pkl')\n",
    "EXON_SEQUENCE_PATH = MATERIALS_PATH.joinpath('gene_symbol_dna_sequence_exon.pkl')\n",
    "UNSPLICED_SEQUENCE_PATH = MATERIALS_PATH.joinpath('gene_symbol_dna_sequence_unspliced.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(GENE_SYMBOL_EFFECT_SIZE)\n",
    "\n",
    "df = df.groupby(['gene_symbol_harmonized'])[['est_m_ea']].agg('mean')\n",
    "\n",
    "df = df.reset_index()\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=RANDOM_STATE).fit(df[['est_m_ea']].to_numpy())\n",
    "\n",
    "df['class'] = kmeans.labels_\n",
    "\n",
    "assert not df['gene_symbol_harmonized'].duplicated().any()\n",
    "\n",
    "_df_effect_size = df.copy()\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(PROTEIN_SEQUENCE_PATH)\n",
    "\n",
    "df = df.rename({'seq': 'sequence'}, axis=1)\n",
    "\n",
    "df = df.groupby(['gene_symbol_harmonized'])[['sequence']].agg(lambda x: ' '.join(x.tolist()))\n",
    "\n",
    "df = df.reset_index()\n",
    "\n",
    "df = _df_effect_size.merge(df, how='inner')\n",
    "\n",
    "df = df[['gene_symbol_harmonized', 'est_m_ea', 'class', 'sequence']]\n",
    "\n",
    "print(df['class'].value_counts())\n",
    "\n",
    "assert not df['gene_symbol_harmonized'].duplicated().any()\n",
    "\n",
    "_df_protein = df.copy()\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(EXON_SEQUENCE_PATH)\n",
    "\n",
    "df = df.rename({'Sequence': 'sequence', 'Gene name': 'gene_symbol_harmonized'}, axis=1)\n",
    "\n",
    "df = df.groupby(['gene_symbol_harmonized'])[['sequence']].agg(lambda x: ' '.join(x.tolist()))\n",
    "\n",
    "df = df.reset_index()\n",
    "\n",
    "df = _df_effect_size.merge(df, how='inner')\n",
    "\n",
    "df = df[['gene_symbol_harmonized', 'est_m_ea', 'class', 'sequence']]\n",
    "\n",
    "print(df['class'].value_counts())\n",
    "\n",
    "assert not df['gene_symbol_harmonized'].duplicated().any()\n",
    "\n",
    "_df_exon = df.copy()\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(UNSPLICED_SEQUENCE_PATH)\n",
    "\n",
    "df = df.rename({'Sequence': 'sequence', 'Gene name': 'gene_symbol_harmonized'}, axis=1)\n",
    "\n",
    "df = df.groupby(['gene_symbol_harmonized'])[['sequence']].agg(lambda x: ' '.join(x.tolist()))\n",
    "\n",
    "df = df.reset_index()\n",
    "\n",
    "df = _df_effect_size.merge(df, how='inner')\n",
    "\n",
    "df = df[['gene_symbol_harmonized', 'est_m_ea', 'class', 'sequence']]\n",
    "\n",
    "print(df['class'].value_counts())\n",
    "\n",
    "assert not df['gene_symbol_harmonized'].duplicated().any()\n",
    "\n",
    "_df_unspliced = df.copy()\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = _df_protein.copy()\n",
    "\n",
    "df = df.rename({'est_m_ea': 'Effect Size', 'class': 'Class'}, axis=1)\n",
    "\n",
    "df['Protein'] = ''\n",
    "\n",
    "_df_protein_strip_plot = df.copy()\n",
    "\n",
    "\n",
    "\n",
    "df = _df_exon.copy()\n",
    "\n",
    "df = df.rename({'est_m_ea': 'Effect Size', 'class': 'Class'}, axis=1)\n",
    "\n",
    "df['Exon'] = ''\n",
    "\n",
    "_df_exon_strip_plot = df.copy()\n",
    "\n",
    "\n",
    "\n",
    "df = _df_unspliced.copy()\n",
    "\n",
    "df = df.rename({'est_m_ea': 'Effect Size', 'class': 'Class'}, axis=1)\n",
    "\n",
    "df['Unspliced'] = ''\n",
    "\n",
    "_df_unspliced_strip_plot = df.copy()\n",
    "\n",
    "\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3,1)\n",
    "\n",
    "fig.suptitle('KMeans Clustering', fontsize='xx-large')\n",
    "fig.set_size_inches((10,4.5))\n",
    "fig.tight_layout()\n",
    "\n",
    "_ = sns.stripplot(data=_df_protein_strip_plot, x='Effect Size', y='Protein', hue=\"Class\", marker='.', jitter=True, s=2, ax=ax1)\n",
    "_= ax1.legend(loc='right')\n",
    "_ = sns.stripplot(data=_df_exon_strip_plot, x='Effect Size', y='Exon', hue=\"Class\", marker='.', jitter=True, s=2,  ax=ax2)\n",
    "_= ax2.legend(loc='right')\n",
    "_ = sns.stripplot(data=_df_unspliced_strip_plot, x='Effect Size', y='Unspliced', hue=\"Class\", marker='.', jitter=True, s=2,  ax=ax3)\n",
    "_= ax3.legend(loc='right')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.75)\n",
    "plt.savefig(RESULTS_PATH.joinpath(f'kmeans_clustering_{RANDOM_STATE}.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_density(df, ngram_range):\n",
    "\n",
    "    tfidf = TfidfVectorizer(analyzer='char_wb', ngram_range=ngram_range)\n",
    "\n",
    "    X = tfidf.fit_transform(df['sequence'].tolist())\n",
    "\n",
    "    X = X.todense()\n",
    "\n",
    "    df = pd.DataFrame(X, columns=tfidf.get_feature_names_out())\n",
    "\n",
    "    df = df.T.copy()\n",
    "\n",
    "    kmeans = KMeans(n_clusters=3, random_state=RANDOM_STATE).fit(df.to_numpy())\n",
    "\n",
    "    df['Class'] = kmeans.labels_\n",
    "\n",
    "    print(pd.Series(kmeans.labels_).value_counts())\n",
    "\n",
    "    df = df.groupby(['Class']).apply(lambda x: ((x != 0).sum().sum()/(x.shape[0] * x.shape[1]), x.index.tolist())).to_frame(name='density_features').reset_index()\n",
    "\n",
    "    df['Density'] = df.apply(lambda x: x['density_features'][0], axis=1)\n",
    "\n",
    "    df['Features'] = df.apply(lambda x: x['density_features'][1], axis=1)\n",
    "\n",
    "    df['Count'] = df['Features'].apply(lambda x: len(x))\n",
    "\n",
    "    df = df.drop(['density_features'], axis=1)\n",
    "\n",
    "    return df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_protein_features = feature_density(df=_df_protein.copy(), ngram_range=(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_exon_features = feature_density(df=_df_exon.copy(), ngram_range=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_unspliced_features = feature_density(df=_df_unspliced.sample(frac=.1, random_state=RANDOM_STATE).copy(), ngram_range=(12,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "\n",
    "ax1.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "ax2.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "ax2.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "\n",
    "fig.suptitle('Feature Density Analysis', fontsize='xx-large')\n",
    "fig.set_size_inches((20,5))\n",
    "fig.subplots_adjust(top=.85)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "sns.barplot(data=_df_protein_features.copy(), x='Class', y='Count', alpha=0.5, ax=ax1)\n",
    "ax1.set_yscale(\"log\")\n",
    "ax1.set_title('Protein Feature Density')\n",
    "ax1_twin = ax1.twinx()\n",
    "sns.scatterplot(data=_df_protein_features, x='Class', y='Density', marker='d', label='Density', color = 'red', s=150, ax=ax1_twin)\n",
    "_= ax1_twin.legend(loc='upper right')\n",
    "\n",
    "sns.barplot(data=_df_exon_features.copy(), x='Class', y='Count', alpha=0.5, ax=ax2)\n",
    "ax2.set_yscale(\"log\")\n",
    "ax2.set_title('Exon Feature Density')\n",
    "ax2_twin = ax2.twinx()\n",
    "sns.scatterplot(data=_df_exon_features, x='Class', y='Density', marker='d', label='Density', color = 'red', s=150, ax=ax2_twin)\n",
    "_= ax2_twin.legend(loc='upper right')\n",
    "\n",
    "sns.barplot(data=_df_unspliced_features.copy(), x='Class', y='Count', alpha=0.5, ax=ax3)\n",
    "ax3.set_yscale(\"log\")\n",
    "ax3.set_title('Unspliced Feature Density')\n",
    "ax3_twin = ax3.twinx()\n",
    "sns.scatterplot(data=_df_unspliced_features, x='Class', y='Density', marker='d', label='Density', color = 'red', s=150, ax=ax3_twin)\n",
    "_= ax3_twin.legend(loc='upper right')\n",
    "\n",
    "plt.savefig(RESULTS_PATH.joinpath(f'feature_density_analysis_{RANDOM_STATE}.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df, ngram_range, vocabulary):\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    X = df['sequence'].to_numpy()\n",
    "    y = df['class'].to_numpy()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # KNeighborsClassifier\n",
    "    param_grid = {\n",
    "    'tfidfVectorizer__ngram_range': [ngram_range],\n",
    "    'tfidfVectorizer__norm': ('l1', 'l2'),\n",
    "    'kNeighborsClassifier__n_neighbors': [3, 5, 9]\n",
    "    }\n",
    "\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('tfidfVectorizer',  TfidfVectorizer(analyzer='char_wb', vocabulary=vocabulary)),\n",
    "        ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "        ('kNeighborsClassifier', KNeighborsClassifier())\n",
    "        ])\n",
    "\n",
    "    kNeighborsClassifier = GridSearchCV(estimator=pipe, param_grid=param_grid, n_jobs=5, verbose=3)\n",
    "\n",
    "    kNeighborsClassifier.fit(X_train, y_train)\n",
    "\n",
    "    accuracy_score = kNeighborsClassifier.score(X_test, y_test)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'classifier': 'KNeighborsClassifier',\n",
    "        'best_params_': [kNeighborsClassifier.best_params_],\n",
    "        'best_score_': kNeighborsClassifier.best_score_,\n",
    "        'test_score': accuracy_score\n",
    "    })\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # SVC\n",
    "    param_grid = {\n",
    "        'tfidfVectorizer__ngram_range': [ngram_range],\n",
    "        'tfidfVectorizer__norm': ('l1', 'l2'),\n",
    "        'SVC__kernel': ['linear', 'rbf'],\n",
    "        'SVC__C': [0.001,0.01,0.1,1,10,100, 1000],\n",
    "        'SVC__gamma': ['scale', 'auto']\n",
    "    }\n",
    "\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('tfidfVectorizer',  TfidfVectorizer(analyzer='char_wb', vocabulary=vocabulary)),\n",
    "        ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "        ('SVC', SVC())\n",
    "        ])\n",
    "\n",
    "    gsSVC = GridSearchCV(estimator=pipe, param_grid=param_grid, n_jobs=5, verbose=3)\n",
    "\n",
    "    gsSVC.fit(X_train, y_train)\n",
    "\n",
    "    accuracy_score = gsSVC.score(X_test, y_test)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'classifier': 'SVC',\n",
    "        'best_params_': [gsSVC.best_params_],\n",
    "        'best_score_': gsSVC.best_score_,\n",
    "        'test_score': accuracy_score\n",
    "    })\n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # LogisticRegression\n",
    "    param_grid = {\n",
    "    'tfidfVectorizer__ngram_range': [ngram_range],\n",
    "    'tfidfVectorizer__norm': ('l1', 'l2'),\n",
    "    'logisticRegression__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "    }\n",
    "\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('tfidfVectorizer',  TfidfVectorizer(analyzer='char_wb', vocabulary=vocabulary)),\n",
    "        ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "        ('logisticRegression', LogisticRegression())\n",
    "        ])\n",
    "\n",
    "    gsLogisticRegression = GridSearchCV(estimator=pipe, param_grid=param_grid, n_jobs=5, verbose=3)\n",
    "\n",
    "    gsLogisticRegression.fit(X_train, y_train)\n",
    "\n",
    "    accuracy_score = gsLogisticRegression.score(X_test, y_test)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'classifier': 'LogisticRegression',\n",
    "        'best_params_': [gsLogisticRegression.best_params_],\n",
    "        'best_score_': gsLogisticRegression.best_score_,\n",
    "        'test_score': accuracy_score\n",
    "    })\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # DummyClassifier\n",
    "    param_grid = {\n",
    "        'tfidfVectorizer__ngram_range': [ngram_range],\n",
    "        'dummyClassifier__strategy': ['uniform', 'most_frequent']\n",
    "    }\n",
    "\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('tfidfVectorizer',  TfidfVectorizer(analyzer='char_wb', vocabulary=vocabulary)), \n",
    "        ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "        ('dummyClassifier', DummyClassifier())\n",
    "        ])\n",
    "\n",
    "    gsDummyClassifier = GridSearchCV(estimator=pipe, param_grid=param_grid, n_jobs=5, verbose=3)\n",
    "\n",
    "    gsDummyClassifier.fit(X_train, y_train)\n",
    "\n",
    "    accuracy_score = gsDummyClassifier.score(X_test, y_test)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'classifier': 'DummyClassifier',\n",
    "        'best_params_': [gsDummyClassifier.best_params_],\n",
    "        'best_score_': gsDummyClassifier.best_score_,\n",
    "        'test_score': accuracy_score\n",
    "    })\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "\n",
    "    df = df.set_index(['classifier']).reset_index()\n",
    "\n",
    "    return df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = _df_protein_features.copy()\n",
    "\n",
    "df = df.loc[df['Density'] != df['Density'].min()]\n",
    "\n",
    "protein_vocabulary = df['Features'].sum()\n",
    "\n",
    "df = _df_protein.copy()\n",
    "\n",
    "df = process(df, (4,4), vocabulary=protein_vocabulary)\n",
    "\n",
    "df.to_pickle(RESULTS_PATH.joinpath(f'classification_protein_{RANDOM_STATE}.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = _df_exon_features.copy()\n",
    "\n",
    "df = df.loc[df['Density'] != df['Density'].min()]\n",
    "\n",
    "exon_vocabulary = df['Features'].sum()\n",
    "\n",
    "df = _df_exon.copy()\n",
    "\n",
    "df = process(df, (10,10), vocabulary=exon_vocabulary)\n",
    "\n",
    "df.to_pickle(RESULTS_PATH.joinpath(f'classification_exon_{RANDOM_STATE}.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = _df_unspliced_features.copy()\n",
    "\n",
    "df = df.loc[df['Density'] != df['Density'].min()]\n",
    "\n",
    "unspliced_vocabulary = df['Features'].sum()\n",
    "\n",
    "df = _df_exon.copy()\n",
    "\n",
    "df = _df_unspliced.copy()\n",
    "\n",
    "df = process(df, (12,12), vocabulary=unspliced_vocabulary)\n",
    "\n",
    "df.to_pickle(RESULTS_PATH.joinpath(f'classification_unspliced_{RANDOM_STATE}.pkl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Development",
   "language": "python",
   "name": "development"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2d6514f9963693ddf9848f840dcbc46c3e7b9abcac09ab97b6beeb9a964f0bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
